{
  "comments": [
    {
      "unresolved": false,
      "key": {
        "uuid": "8ec4a618_0537a194",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 2
      },
      "lineNbr": 0,
      "author": {
        "id": 1884045
      },
      "writtenOn": "2024-01-09T19:26:33Z",
      "side": 1,
      "message": "Adding Jared and Tim to check if I\u0027m doing the caching correctly.",
      "revId": "ec317e96825d657de911d44da98d712c33d659d7",
      "serverId": "85c56323-6fa9-3386-8a01-6480fb634889"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "e2eaefe9_72c59fd3",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 2
      },
      "lineNbr": 0,
      "author": {
        "id": 1064003
      },
      "writtenOn": "2024-01-09T19:46:22Z",
      "side": 1,
      "message": "Thanks for taking a look at this Jiakai. What are your thoughts on the additional complexity of this approach versus something a bit more narrowly scoped and simple like aosp/2889793? I\u0027m somewhat indifferent, though I\u0027m always a bit hesitant of solutions that have timeout-based expirations.",
      "revId": "ec317e96825d657de911d44da98d712c33d659d7",
      "serverId": "85c56323-6fa9-3386-8a01-6480fb634889"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "ad192d23_249d7ba7",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 2
      },
      "lineNbr": 0,
      "author": {
        "id": 1884045
      },
      "writtenOn": "2024-01-09T23:53:11Z",
      "side": 1,
      "message": "Thanks for the thoughts. For b/317221732 (just avoiding repetitive waitForService calls), I think both solutions work, and I would prefer aosp/2889793 over this CL because it\u0027s simpler.\n\nHowever, when it comes to avoiding repetitive artd initialization, which is much more expensive than repetitive waitForService calls, the pinning approach in this CL is necessary. aosp/2889793 basically keeps the behavior unchanged, which relies on an assumption that the GC doesn\u0027t happen immediately after an artd handle is no longer referenced. I don\u0027t know how well this assumption holds in reality. Maybe under extreme memory pressure, artd is brought up and teared down repetitively because of the GC, though this is just theoretical. This has been my concern for a long time. I\u0027m actually more comfortable with explicit pinning instead of relying on the assumption.\n\nIn short, I think aosp/2889793 is simpler and it solves one problem, while this CL is more complicated but solves two at a time, though one of them is theoretical. But TBH, I\u0027m indifferent too. I\u0027ll leave this to Martin to provide some thoughts.\n\nThe timeout-based expiration is just a bonus. This main part of ArtdRefCache is about pinning. Because we already have a debouncer implementation, it\u0027s easy to add a few lines to use it to implement the timeout-based expiration. It can fight against the case where our APIs are called repetitively, in which case a local pinning won\u0027t work. I\u0027m interested in seeing how our APIs are used in real CUJs. You mentioned initial device setup in aosp/2889793. Could you provide some traces to show how our APIs are used in that CUJ? If it turns out that the timeout-based expiration is useless, I can remove it.",
      "parentUuid": "e2eaefe9_72c59fd3",
      "revId": "ec317e96825d657de911d44da98d712c33d659d7",
      "serverId": "85c56323-6fa9-3386-8a01-6480fb634889"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "ba933361_d833f959",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 2
      },
      "lineNbr": 0,
      "author": {
        "id": 1373864
      },
      "writtenOn": "2024-01-10T21:31:21Z",
      "side": 1,
      "message": "I think the unpredictable GC behaviour on high memory pressure is a good point. It\u0027s nice to avoid that uncertainty.\n\nThat said, the ref cache is rather more code than I\u0027d like for that sort of thing. I\u0027d expect/hope the JDK would have something to help with such scoped caching. That\u0027s not the case?\n\n\u003e it\u0027s easy to add a few lines to use it to implement the timeout-based expiration\n\nWell, it also introduces the finicky `DeathRecipient` stuff, doesn\u0027t it? That makes it not so trivial anymore.\n\nIf we do find CUJ\u0027s where there\u0027s a tangible benefit to cache the artd instance longer, then the proper approach is arguably to introduce a session class to give control of the lifetime to the caller (but admittedly at the cost of adding quite a bit of new API).\n\nAnother idea is to combine this and aosp/2889793 - i.e. do the refcounting but leave behind a best-effort weak ref when the refcount goes to zero. It\u0027s not clear to me if a weak ref would avoid the `DeathRecipient` stuff though - isn\u0027t it also prone to cache a potentially dead instance?",
      "parentUuid": "ad192d23_249d7ba7",
      "revId": "ec317e96825d657de911d44da98d712c33d659d7",
      "serverId": "85c56323-6fa9-3386-8a01-6480fb634889"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "5ff39e07_eb424726",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 2
      },
      "lineNbr": 0,
      "author": {
        "id": 1884045
      },
      "writtenOn": "2024-01-10T21:58:07Z",
      "side": 1,
      "message": "\u003e That said, the ref cache is rather more code than I\u0027d like for that sort of thing.\n\nThe file looks lengthy because it has various boilerplates like license, imports, injector pattern, etc. The core part is line 80-153 (only 74 lines).\n\n\u003e I\u0027d expect/hope the JDK would have something to help with such scoped caching. That\u0027s not the case?\n\nI\u0027d not expect JDK to have anything smart, frankly speaking :)\n\n\u003e it also introduces the finicky DeathRecipient stuff, doesn\u0027t it?\n\n`DeathRecipient` is not for the timeout-based expiration. It is needed whenever we keep a ref, so that if a previous call causes a crash, the next call won\u0027t be affected. For example, during bg dexopt, if artd crashes when dexopting a package, we still want the job to continue to dexopt other packages. Even if we decide to adopt aosp/2889793 (the `WeakReference` approach), we\u0027ll have to add `DeathRecipient` before submitting the CL.\n\nBTW, it\u0027s unfortunate that the `DeathRecipient` has to be a class because it has two callbacks, one of which is legacy. Otherwise, I could make it a lambda so that it looks shorter. But this is just an API detail. The lengthy class doesn\u0027t really add logically complexity, so don\u0027t be scared of it. At the end of the day, it\u0027s just a callback with 5 lines of code.",
      "parentUuid": "ba933361_d833f959",
      "revId": "ec317e96825d657de911d44da98d712c33d659d7",
      "serverId": "85c56323-6fa9-3386-8a01-6480fb634889"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "7e6c8c1c_a1d2ff83",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 2
      },
      "lineNbr": 0,
      "author": {
        "id": 1064003
      },
      "writtenOn": "2024-01-10T22:08:22Z",
      "side": 1,
      "message": "\u003e Even if we decide to adopt aosp/2889793 (the WeakReference approach), we\u0027ll have to add DeathRecipient before submitting the CL.\n\nDefinitely. @smoreland@google.com would you be the right person to talk in brainstorming how we could handle this more seamlessly for arbitrary clients? Maybe ART\u0027s needs are slightly unique here in how it\u0027s referencing the transient `artd` service, but I wonder if we can solve this more generally in the platform in a way that minimizes unnecessary calls into `servicemanager` for these handles (without needing each client to add this kind of additional caching/bookkeeping logic).",
      "parentUuid": "5ff39e07_eb424726",
      "revId": "ec317e96825d657de911d44da98d712c33d659d7",
      "serverId": "85c56323-6fa9-3386-8a01-6480fb634889"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "16e1e0ac_8031bad4",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 2
      },
      "lineNbr": 0,
      "author": {
        "id": 1373864
      },
      "writtenOn": "2024-01-10T22:37:49Z",
      "side": 1,
      "message": "\u003e Even if we decide to adopt aosp/2889793 (the WeakReference approach), we\u0027ll have to add DeathRecipient before submitting the CL.\n\nHow can we be sure about that, doesn\u0027t it depend on why artd died? If it got killed by the LMK then it sounds like a good idea to back off - dexopting isn\u0027t _that_ important. If it crashed because something is broken then isn\u0027t it rather likely that it\u0027ll just crash soon enough again? (dex2oat and profman are a different matter in that regard.)\n\n\u003e The lengthy class doesn\u0027t really add logically complexity\n\nI\u0027m discussing logical complexity, not lines. `CacheDeathRecipient` is indeed tedious and annoying, but it\u0027s line 91 that\u0027s the issue.",
      "parentUuid": "7e6c8c1c_a1d2ff83",
      "revId": "ec317e96825d657de911d44da98d712c33d659d7",
      "serverId": "85c56323-6fa9-3386-8a01-6480fb634889"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "1450f9ff_7f0e2041",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 2
      },
      "lineNbr": 0,
      "author": {
        "id": 1884045
      },
      "writtenOn": "2024-01-11T12:57:30Z",
      "side": 1,
      "message": "\u003e If it got killed by the LMK then it sounds like a good idea to back off\n\nartd is a native service, so it gets the lowest oom_adj (NATIVE_ADJ), which means it\u0027s in the last category that LMK would kill. If we want to respect low memory we\u0027d rely on something else. For example, job scheduler probably cancels some jobs on low memory, and we do respect job cancellation. artd can only die because it goes wrong.\n\n\u003e If it crashed because something is broken then isn\u0027t it rather likely that it\u0027ll just crash soon enough again?\n\nI beg to differ. Maybe an app with an extremely long CLC can crash artd (just saying, not the real case), in which case artd won\u0027t crash on other apps.\n\n\u003e but it\u0027s line 91 that\u0027s the issue.\n\nSorry, I don\u0027t follow. When artd is dead, CacheDeathRecipient just clears the cache, as if there wasn\u0027t any cached artd reference. We have code that checks the existence of the cache and build it if it doesn\u0027t exist, for the very first `getArtd()` call, so the same code handles the death case naturally. Or, are you concerning about some other logical complexity?",
      "parentUuid": "16e1e0ac_8031bad4",
      "revId": "ec317e96825d657de911d44da98d712c33d659d7",
      "serverId": "85c56323-6fa9-3386-8a01-6480fb634889"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "cc2649dd_d4b8e474",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 2
      },
      "lineNbr": 0,
      "author": {
        "id": 1373864
      },
      "writtenOn": "2024-01-11T21:29:11Z",
      "side": 1,
      "message": "\u003e Maybe an app with an extremely long CLC can crash artd (just saying, not the real case), in which case artd won\u0027t crash on other apps.\n\nApp specific failure mode is a factor for profman and dex2oat invocations, but I remain unconvinced that it\u0027s a likely one for artd itself. It\u0027s much more likely with a resource contention of some sort, or a code bug that\u0027s triggered by some condition in the execution environment. But we\u0027re both guessing here.\n\nBe that as it may, the impact of consistently failing to progress past some point is arguably worse than of getting a cascade of artd crashes (except maybe if they can DoS the system, but I\u0027m going to assume that the crash dumper has rate limiting measures in place to handle crash loops). So on that ground I can agree that `DeathRecipient` should be used.\n\nAfter some thought I\u0027m less keen on timed expiration (in any form) though - it\u0027s after all a premature optimisation, and there are different options to consider if we want to do it.",
      "parentUuid": "1450f9ff_7f0e2041",
      "revId": "ec317e96825d657de911d44da98d712c33d659d7",
      "serverId": "85c56323-6fa9-3386-8a01-6480fb634889"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "664d1b4c_8c5c87e3",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 2
      },
      "lineNbr": 0,
      "author": {
        "id": 1884045
      },
      "writtenOn": "2024-01-11T22:38:19Z",
      "side": 1,
      "message": "\u003e App specific failure mode is a factor for profman and dex2oat invocations, but I remain unconvinced that it\u0027s a likely one for artd itself. [...]\n\nNot really. App-specific failure mode can also happen on artd itself. I\u0027m not guessing. I\u0027m saying that this is a theoretical possibility that this can happen. For example, during dexopt, we [parse the CLC](https://cs.android.com/android/platform/superproject/main/+/main:art/artd/artd.cc;l\u003d987;drc\u003d239f3b0767a9c691a5ba9949f343d96651c9280b) in order to get the paths to the dependencies. Something in the parser can go wrong and cause a crash when the CLC contains a certain pattern. Of course, this can only happen when there is a bug in the code. We wouldn\u0027t need the `DeathRecipient` if the code is bug-free. However, I don\u0027t think we can ever say that our code is bug-free.\n\n\u003e After some thought I\u0027m less keen on timed expiration (in any form) though - it\u0027s after all a premature optimisation, and there are different options to consider if we want to do it.\n\nI remember that we saw a test, during which artd is repeatedly started and stopped due to the GC (see logs attached to b/278038751#comment1). https://screenshot.googleplex.com/5DgC8aYgqzducLS\n\nI don\u0027t know what the exact test is, but I suspect it\u0027s [cts/hostsidetests/appsecurity/src/android/appsecurity/cts/SplitTests.java](https://cs.android.com/android/platform/superproject/main/+/main:cts/hostsidetests/appsecurity/src/android/appsecurity/cts/SplitTests.java). Looks like it installs various apks, and hence triggered the repeated start-and-stop.\n\nThis reminds me that it may also happen in reality: when the user clicks \"update all\" in Play Store, when the device is restoring from a backup, or when the user sets up a work profile, a burst of installs take place. Therefore, I see a value in a timeout-based expiration.",
      "parentUuid": "cc2649dd_d4b8e474",
      "revId": "ec317e96825d657de911d44da98d712c33d659d7",
      "serverId": "85c56323-6fa9-3386-8a01-6480fb634889"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "80fc71cb_47b5ac8d",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 2
      },
      "lineNbr": 0,
      "author": {
        "id": 1120458
      },
      "writtenOn": "2024-01-13T00:15:06Z",
      "side": 1,
      "message": "\u003e without needing each client to add this kind of additional caching/bookkeeping logic\n\nMissed this when you originally added me, and I didn\u0027t realize you were handling the lazy cases. I don\u0027t really know what way of managing this would actually be generally useful. I would personally always use an object to act as the session so that I wouldn\u0027t have this problem, but if you have a proposal or other ideas, happy to talk about it. Obviously, the art case would need a lot of other refactoring to keep track of this in the type system.",
      "parentUuid": "664d1b4c_8c5c87e3",
      "revId": "ec317e96825d657de911d44da98d712c33d659d7",
      "serverId": "85c56323-6fa9-3386-8a01-6480fb634889"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "8ea9040b_6276144b",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 2
      },
      "lineNbr": 0,
      "author": {
        "id": 1373864
      },
      "writtenOn": "2024-01-15T01:18:35Z",
      "side": 1,
      "message": "\u003e Not really. App-specific failure mode can also happen /.../\n\nSure, didn\u0027t say otherwise. I can also start listing examples when it doesn\u0027t matter, but it\u0027d be a pointless discussion.\n\n\u003e This reminds me that it may also happen in reality: when the user clicks \"update all\" in Play Store, when the device is restoring from a backup, or when the user sets up a work profile, a burst of installs take place. Therefore, I see a value in a timeout-based expiration.\n\nI don\u0027t think tests are relevant, but some of these CUJs may be. I don\u0027t know if fetching a service handle about once per package is heavy enough to matter in them - maybe @jdduke@google.com has an opinion on that?\n\n\u003e Obviously, the art case would need a lot of other refactoring to keep track of this in the type system.\n\nYes, exposing a session object would probably need a fair bit of new API, and I suppose it\u0027d also be a bit of work to pass it around in the framework code if we need to handle the multi-package CUJs Jiakai mentioned.",
      "parentUuid": "80fc71cb_47b5ac8d",
      "revId": "ec317e96825d657de911d44da98d712c33d659d7",
      "serverId": "85c56323-6fa9-3386-8a01-6480fb634889"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "b7fc3c1d_18ecc549",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 2
      },
      "lineNbr": 0,
      "author": {
        "id": 1884045
      },
      "writtenOn": "2024-01-16T14:44:11Z",
      "side": 1,
      "message": "\u003e I don\u0027t know if fetching a service handle about once per package is heavy enough to matter in them\n\nI don\u0027t think fetching a service handle once per package is a problem, but I think initializing artd once per package is, and that\u0027s what is happening during that test, if you see the logs in that ticket.\n\nI\u0027m not saying tests are relevant, but I\u0027m saying that that test does nothing special but installs many packages, which can also happen in some real CUJs, so it concerns me.\n\nExposing a session object is a proper way to handle this case, but as you said, it is too heavy-weight. That\u0027s why I prefer a timeout-based expiration, which is relatively light-weight compared to that, and it should handle the case quite well in practice, though it\u0027s not an elegant solution. WDYT?",
      "parentUuid": "8ea9040b_6276144b",
      "revId": "ec317e96825d657de911d44da98d712c33d659d7",
      "serverId": "85c56323-6fa9-3386-8a01-6480fb634889"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "4bc9e375_01b147d6",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 2
      },
      "lineNbr": 0,
      "author": {
        "id": 1373864
      },
      "writtenOn": "2024-01-18T21:14:05Z",
      "side": 1,
      "message": "\u003e I think initializing artd once per package is, and that\u0027s what is happening during that test\n\nInteresting, I got a different picture from http://b/317221732#comment1. But I suppose it\u0027s WAI that the process is terminated immediately when the service handle is finalised.",
      "parentUuid": "b7fc3c1d_18ecc549",
      "revId": "ec317e96825d657de911d44da98d712c33d659d7",
      "serverId": "85c56323-6fa9-3386-8a01-6480fb634889"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "a55fc0e8_156668df",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 2
      },
      "lineNbr": 0,
      "author": {
        "id": 1884045
      },
      "writtenOn": "2024-01-19T12:31:40Z",
      "side": 1,
      "message": "Looks like we were talking about different things. http://b/317221732#comment1 shows the service handle being fetched multiple times. http://b/278038751#comment1 shows artd being killed and restarted multiple times. I was saying that the latter happened during a test, which does nothing special but installing a bunch of apps, which can also happen in real CUJs.",
      "parentUuid": "4bc9e375_01b147d6",
      "revId": "ec317e96825d657de911d44da98d712c33d659d7",
      "serverId": "85c56323-6fa9-3386-8a01-6480fb634889"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "a43b86c2_833d4560",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 2
      },
      "lineNbr": 0,
      "author": {
        "id": 1373864
      },
      "writtenOn": "2024-01-19T16:12:57Z",
      "side": 1,
      "message": "My point is that the test appears to show that the former (dropping the service handle) implies the latter (artd is killed right away). If that\u0027s the case it\u0027s a lot more heavy-weight than I thought it was. If it\u0027s not intentional then someone on the framework side ought to take a look, and we probably shouldn\u0027t be working around it at this level. But I\u0027ll assume it is intentional.",
      "parentUuid": "a55fc0e8_156668df",
      "revId": "ec317e96825d657de911d44da98d712c33d659d7",
      "serverId": "85c56323-6fa9-3386-8a01-6480fb634889"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "0f7d3614_501a3b5d",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 2
      },
      "lineNbr": 0,
      "author": {
        "id": 1884045
      },
      "writtenOn": "2024-01-19T17:11:28Z",
      "side": 1,
      "message": "Dropping the service handle does not make artd killed right away. Though I believe killing art service right away is the desired behavior, Java doesn\u0027t offer a C++-style destructor that can make this possible. In fact, artd is killed by the service handle finalizer, so it very depends on the timing of the GC. I would say the behavior is that artd is **eventually** killed after the service handle is dropped.\n\nI think the stack in http://b/317221732#comment1 shows a light-weight situation, where the GC was not that aggressive, while the logs in http://b/278038751#comment1 shows a heavy-weight situation, where the GC was aggressive. Both cases can happen in reality, depending on the memory pressure.\n\nBecause both cases can happen, we need to optimize for the worse case. That\u0027s why I think a timeout-based expiration is necessary.",
      "parentUuid": "a43b86c2_833d4560",
      "revId": "ec317e96825d657de911d44da98d712c33d659d7",
      "serverId": "85c56323-6fa9-3386-8a01-6480fb634889"
    }
  ]
}