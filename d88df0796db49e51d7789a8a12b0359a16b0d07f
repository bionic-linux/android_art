{
  "comments": [
    {
      "unresolved": true,
      "key": {
        "uuid": "6f447939_107c38c6",
        "filename": "dex2oat/linker/code_info_table_deduper.cc",
        "patchSetId": 9
      },
      "lineNbr": 24,
      "author": {
        "id": 1065473
      },
      "writtenOn": "2022-03-08T16:24:38Z",
      "side": 1,
      "message": "This method grew a lot!\n\nIs it really needed to do in so many passes?",
      "revId": "d88df0796db49e51d7789a8a12b0359a16b0d07f",
      "serverId": "85c56323-6fa9-3386-8a01-6480fb634889"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "d32df127_ca817ebb",
        "filename": "dex2oat/linker/code_info_table_deduper.cc",
        "patchSetId": 9
      },
      "lineNbr": 24,
      "author": {
        "id": 1018108
      },
      "writtenOn": "2022-03-08T17:10:55Z",
      "side": 1,
      "message": "Well, each bit has its purpose.\n\nUnlike `std::map\u003c\u003e`, the `HashSet\u003c\u003e` can move entries, so we need to pre-allocate.\n\nTo reduce the memory usage, the `DedupeSetEntry` contains only bit offset and bit size in `output` data, so we need to copy the data to `output` before inserting entries in the `HashSet\u003c\u003e`. For performance reasons we prefer a single `writer_.WriteRegion(read_region)` instead of writing individual tables which means we need separate sections of code for decoding the code info and populating the `dedupe_set_`.",
      "parentUuid": "6f447939_107c38c6",
      "revId": "d88df0796db49e51d7789a8a12b0359a16b0d07f",
      "serverId": "85c56323-6fa9-3386-8a01-6480fb634889"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "2308002a_24045bcb",
        "filename": "dex2oat/linker/code_info_table_deduper.cc",
        "patchSetId": 9
      },
      "lineNbr": 24,
      "author": {
        "id": 1065473
      },
      "writtenOn": "2022-03-08T18:18:13Z",
      "side": 1,
      "message": "Hmmm... but shouldn\u0027t it all still be possible in single pass? (same as it was before)  Or similar - I feel like hashing during the CodeInfo read might be simpler.\n\nWhat was wrong with having the BitMemoryRegion as (part of) key? (internally it is also just bit offset+size as well).  I mean, understand we need some extra work if we want to calculate the hash just once, but on other it shouldn\u0027t fundamentally change the simple single-pass algorithm, right?\n\nThe copy to writer seems like unnecessary extra work, and has hidden issue that writing one table can spill into following one that is then read (which is easy to fix though).\n\nPerformance wise, we can still have trivial fast path \"if there no de-dups, just copy everything\".  Is that good enough?",
      "parentUuid": "d32df127_ca817ebb",
      "revId": "d88df0796db49e51d7789a8a12b0359a16b0d07f",
      "serverId": "85c56323-6fa9-3386-8a01-6480fb634889"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "554bacd3_776976a1",
        "filename": "dex2oat/linker/code_info_table_deduper.cc",
        "patchSetId": 9
      },
      "lineNbr": 24,
      "author": {
        "id": 1018108
      },
      "writtenOn": "2022-03-09T09:40:42Z",
      "side": 1,
      "message": "`BitMemoryRegion` itself is `3 * sizeof(size_t)` and you need that additional offset to the `output` which shall yield 16B entries in `dex2oat32` and 32B entries in `dex2oat64`, compared to my 12B entries. And I\u0027d like to pre-allocate a buffer with 100K entries or 1.5MB for the boot image compilation which would then be 2MB or 4MB with the `HashMap\u003cBitMemoryRegion, uint32_t\u003e`. (I\u0027ve seen ~60K actual entries when compiling the whole boot image, ~39K for the extension. Pair that with the max load factor that I\u0027ve set to 0.75 and some extra space for larger profile.)\n\nThe copy to writer is \"unnecessary\" only if there are actual deduplicated tables. (It is in fact necessary for the insertion to the `dedupe_set_` now that entries reference the `output`.)\n\nHashing during the reading of the code info does not buy us anything as the insertion also needs to do actual comparisons with `BitMemoryRegion::Equals()`.\n\nThere is also one subtle problem that prevents some simplification. If one code info has two bit tables with the same bit representation, we want to deduplicate, so we cannot simply search for duplicates during the initial pass (which would be possible with overloaded operators in hash function and equality function) but need to take also newly inserted entries into account.",
      "parentUuid": "2308002a_24045bcb",
      "revId": "d88df0796db49e51d7789a8a12b0359a16b0d07f",
      "serverId": "85c56323-6fa9-3386-8a01-6480fb634889"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "8181b8bd_11e86f78",
        "filename": "dex2oat/linker/code_info_table_deduper.cc",
        "patchSetId": 9
      },
      "lineNbr": 24,
      "author": {
        "id": 1065473
      },
      "writtenOn": "2022-03-09T11:09:17Z",
      "side": 1,
      "message": "I see.  The set memory use is important.  There is a question whether `BitMemoryRegion` needs to be `size_t`.  That is kind of arbitrary.  So I guess the only difference is the removal of buffer pointer from the key.  Fair enough.\n\nOf course there are ways that you could make the entries still point to the read buffer (e.g. one bit in the entry which would mean to use `current_read_buffer_` instead of `output_`).  But that is extra code, and I was arguing for simplicity, so maybe not.\n\nAlso, mirror thought.  If size is of the essence, would 16-bit hash+bit_size do? (i.e. never dedupe 64k+ tables).  I guess we don\u0027t have that big tables.  But then if we ever do, dedupe would be super useful there.  I don\u0027t know.\n\nDo you know how often we have any dupes vs not?  Would it be worth aligning the output before the big copy?  That would make the copy cheaper, but then you would always need to move it.  I don\u0027t know which way the tradeoff goes.\n\nOverall, am I starting to see why it got so complex.  My main though now is why the `DedupeSetEntry` pointer?  Wouldn\u0027t just value do?  In case of movement remove the old, and insert new. Would it have measurable performance difference?",
      "parentUuid": "554bacd3_776976a1",
      "revId": "d88df0796db49e51d7789a8a12b0359a16b0d07f",
      "serverId": "85c56323-6fa9-3386-8a01-6480fb634889"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "4ddc783d_3eb2f5ed",
        "filename": "dex2oat/linker/code_info_table_deduper.cc",
        "patchSetId": 9
      },
      "lineNbr": 24,
      "author": {
        "id": 1065473
      },
      "writtenOn": "2022-03-09T11:11:18Z",
      "side": 1,
      "message": "Or could you lookup the `DedupeSetEntry` by-value (i.e. getting the pointer) and then modify it in-place?",
      "parentUuid": "8181b8bd_11e86f78",
      "revId": "d88df0796db49e51d7789a8a12b0359a16b0d07f",
      "serverId": "85c56323-6fa9-3386-8a01-6480fb634889"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "0ace4152_8e335500",
        "filename": "dex2oat/linker/code_info_table_deduper.cc",
        "patchSetId": 9
      },
      "lineNbr": 24,
      "author": {
        "id": 1018108
      },
      "writtenOn": "2022-03-09T12:41:07Z",
      "side": 1,
      "message": "BitMemoryRegion holds a pointer, so it should really be `alignof(size_t)`. Making the other members `uint32_t` would only reduce the entry size here from 32B to 24B for dex2oat64. I\u0027d prefer to keep those members `size_t`.\n\nThe `HashSet\u003c\u003e` does not have a pointer to `code_info_data` and adding that to the hash/equals structures and modifying it for each `Dedupe()` call is also quite intrusive. (And HashSet\u003c\u003e would need new functions to expose references to hash/equals functions.)\n\nWe could also remove the `hash` from the entry but that means more calls to `Equals()`. Once we pre-allocate a huge table, this should be OK but I\u0027d prefer to keep the member for now.\n\nI\u0027ve recently recorded some statistics on my test device:\next: CodeInfoTableDeduper stats: num_code_infos_\u003d14428 dedupe_set_.size()\u003d38858 output_bits\u003d12673981\nfull: CodeInfoTableDeduper stats: num_code_infos_\u003d20803 dedupe_set_.size()\u003d59693 output_bits\u003d21591133\nGiven `kNumBitTables \u003d 8`, unique bit tables are ~33.7% for extension and ~35.9% for the full boot image.\n\nWe need `DedupeSetEntry*` to update the entry as we rewrite the `output` (for new entries). And while we\u0027re rewriting them, we should not do any lookups in the `HashSet\u003c\u003e` with temporarily invalid contents. See also comment in lines 94-96. I don\u0027t really want to remove and re-insert entries given the statistics above; see `HashSet::InsertWithHash` samples in profiles posted on the bug. (That would also make this function even longer.)",
      "parentUuid": "4ddc783d_3eb2f5ed",
      "revId": "d88df0796db49e51d7789a8a12b0359a16b0d07f",
      "serverId": "85c56323-6fa9-3386-8a01-6480fb634889"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "7adddd0a_7e35b5e3",
        "filename": "dex2oat/linker/code_info_table_deduper.cc",
        "patchSetId": 9
      },
      "lineNbr": 24,
      "author": {
        "id": 1065473
      },
      "writtenOn": "2022-03-09T15:59:22Z",
      "side": 1,
      "message": "I wasn\u0027t suggesting removing the `hash` but making it uint16.  But with 60k entries, by birthday paradox, uint32 is already pretty tight fit. So scratch that comment.\n\nI don\u0027t follow the last comment.  Given `DedupeSetEntry` surely it should be easy enough to look it up and just modify in pace.  Having the pointer is nice, but hashtable lookup should be very cheap too, right?  (note that we probably need to add `lhs.bit_offset \u003d\u003d rhs.bit_offset || BitMemoryRegion::Equals` in `DedupeSetEntryEquals` to avoid the bit region comparison on that path)",
      "parentUuid": "0ace4152_8e335500",
      "revId": "d88df0796db49e51d7789a8a12b0359a16b0d07f",
      "serverId": "85c56323-6fa9-3386-8a01-6480fb634889"
    }
  ]
}