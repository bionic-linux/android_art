{
  "comments": [
    {
      "unresolved": true,
      "key": {
        "uuid": "6f447939_107c38c6",
        "filename": "dex2oat/linker/code_info_table_deduper.cc",
        "patchSetId": 9
      },
      "lineNbr": 24,
      "author": {
        "id": 1065473
      },
      "writtenOn": "2022-03-08T16:24:38Z",
      "side": 1,
      "message": "This method grew a lot!\n\nIs it really needed to do in so many passes?",
      "revId": "d88df0796db49e51d7789a8a12b0359a16b0d07f",
      "serverId": "85c56323-6fa9-3386-8a01-6480fb634889"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "d32df127_ca817ebb",
        "filename": "dex2oat/linker/code_info_table_deduper.cc",
        "patchSetId": 9
      },
      "lineNbr": 24,
      "author": {
        "id": 1018108
      },
      "writtenOn": "2022-03-08T17:10:55Z",
      "side": 1,
      "message": "Well, each bit has its purpose.\n\nUnlike `std::map\u003c\u003e`, the `HashSet\u003c\u003e` can move entries, so we need to pre-allocate.\n\nTo reduce the memory usage, the `DedupeSetEntry` contains only bit offset and bit size in `output` data, so we need to copy the data to `output` before inserting entries in the `HashSet\u003c\u003e`. For performance reasons we prefer a single `writer_.WriteRegion(read_region)` instead of writing individual tables which means we need separate sections of code for decoding the code info and populating the `dedupe_set_`.",
      "parentUuid": "6f447939_107c38c6",
      "revId": "d88df0796db49e51d7789a8a12b0359a16b0d07f",
      "serverId": "85c56323-6fa9-3386-8a01-6480fb634889"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "2308002a_24045bcb",
        "filename": "dex2oat/linker/code_info_table_deduper.cc",
        "patchSetId": 9
      },
      "lineNbr": 24,
      "author": {
        "id": 1065473
      },
      "writtenOn": "2022-03-08T18:18:13Z",
      "side": 1,
      "message": "Hmmm... but shouldn\u0027t it all still be possible in single pass? (same as it was before)  Or similar - I feel like hashing during the CodeInfo read might be simpler.\n\nWhat was wrong with having the BitMemoryRegion as (part of) key? (internally it is also just bit offset+size as well).  I mean, understand we need some extra work if we want to calculate the hash just once, but on other it shouldn\u0027t fundamentally change the simple single-pass algorithm, right?\n\nThe copy to writer seems like unnecessary extra work, and has hidden issue that writing one table can spill into following one that is then read (which is easy to fix though).\n\nPerformance wise, we can still have trivial fast path \"if there no de-dups, just copy everything\".  Is that good enough?",
      "parentUuid": "d32df127_ca817ebb",
      "revId": "d88df0796db49e51d7789a8a12b0359a16b0d07f",
      "serverId": "85c56323-6fa9-3386-8a01-6480fb634889"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "554bacd3_776976a1",
        "filename": "dex2oat/linker/code_info_table_deduper.cc",
        "patchSetId": 9
      },
      "lineNbr": 24,
      "author": {
        "id": 1018108
      },
      "writtenOn": "2022-03-09T09:40:42Z",
      "side": 1,
      "message": "`BitMemoryRegion` itself is `3 * sizeof(size_t)` and you need that additional offset to the `output` which shall yield 16B entries in `dex2oat32` and 32B entries in `dex2oat64`, compared to my 12B entries. And I\u0027d like to pre-allocate a buffer with 100K entries or 1.5MB for the boot image compilation which would then be 2MB or 4MB with the `HashMap\u003cBitMemoryRegion, uint32_t\u003e`. (I\u0027ve seen ~60K actual entries when compiling the whole boot image, ~39K for the extension. Pair that with the max load factor that I\u0027ve set to 0.75 and some extra space for larger profile.)\n\nThe copy to writer is \"unnecessary\" only if there are actual deduplicated tables. (It is in fact necessary for the insertion to the `dedupe_set_` now that entries reference the `output`.)\n\nHashing during the reading of the code info does not buy us anything as the insertion also needs to do actual comparisons with `BitMemoryRegion::Equals()`.\n\nThere is also one subtle problem that prevents some simplification. If one code info has two bit tables with the same bit representation, we want to deduplicate, so we cannot simply search for duplicates during the initial pass (which would be possible with overloaded operators in hash function and equality function) but need to take also newly inserted entries into account.",
      "parentUuid": "2308002a_24045bcb",
      "revId": "d88df0796db49e51d7789a8a12b0359a16b0d07f",
      "serverId": "85c56323-6fa9-3386-8a01-6480fb634889"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "8181b8bd_11e86f78",
        "filename": "dex2oat/linker/code_info_table_deduper.cc",
        "patchSetId": 9
      },
      "lineNbr": 24,
      "author": {
        "id": 1065473
      },
      "writtenOn": "2022-03-09T11:09:17Z",
      "side": 1,
      "message": "I see.  The set memory use is important.  There is a question whether `BitMemoryRegion` needs to be `size_t`.  That is kind of arbitrary.  So I guess the only difference is the removal of buffer pointer from the key.  Fair enough.\n\nOf course there are ways that you could make the entries still point to the read buffer (e.g. one bit in the entry which would mean to use `current_read_buffer_` instead of `output_`).  But that is extra code, and I was arguing for simplicity, so maybe not.\n\nAlso, mirror thought.  If size is of the essence, would 16-bit hash+bit_size do? (i.e. never dedupe 64k+ tables).  I guess we don\u0027t have that big tables.  But then if we ever do, dedupe would be super useful there.  I don\u0027t know.\n\nDo you know how often we have any dupes vs not?  Would it be worth aligning the output before the big copy?  That would make the copy cheaper, but then you would always need to move it.  I don\u0027t know which way the tradeoff goes.\n\nOverall, am I starting to see why it got so complex.  My main though now is why the `DedupeSetEntry` pointer?  Wouldn\u0027t just value do?  In case of movement remove the old, and insert new. Would it have measurable performance difference?",
      "parentUuid": "554bacd3_776976a1",
      "revId": "d88df0796db49e51d7789a8a12b0359a16b0d07f",
      "serverId": "85c56323-6fa9-3386-8a01-6480fb634889"
    }
  ]
}