{
  "comments": [
    {
      "unresolved": false,
      "key": {
        "uuid": "9e9cff68_78149eca",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 1042828
      },
      "writtenOn": "2024-10-22T22:52:18Z",
      "side": 1,
      "message": "I like the code simplification a lot.  I think this approach is probably an improvement, but not ideal. It will be significantly off in some cases:\n\nIf an allocation pattern repeats at the a rate that divides the TLAB size, you\u0027ll get really biased results. I think cputime profilers have seen this effect in the past, and generally randomize sampling as a result. I guess for long-lived threads, the GC should randomize things a bit.\n\nIf you have mostly short-lived threads doing the allocation, you\u0027ll greatly over-count the first allocation, I suspect? The total is also likely to be way high, since the TLABs won\u0027t be fully used. But then this to some extent reflects what\u0027s probably a bit of a GC problem at the moment. System server has O(450) threads. That means they use O(15MB) once they each allocate a single small object. I think we really need to reduce the 32K TLAB size.\n\nLokesh - When a thread dies, do we reuse its TLAB before the next GC? We probably should?",
      "revId": "b0b11e0169dd33a938c67cc25512fc484e43198f",
      "serverId": "85c56323-6fa9-3386-8a01-6480fb634889"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "ab03c885_bef8b9d7",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 1057373
      },
      "writtenOn": "2024-10-23T20:13:19Z",
      "side": 1,
      "message": "Good point. Let me think about whether there is an easy way to avoid the biased results (e.g. blaming a random allocation in the TLAB buffer instead of the first allocation), and understand better how this compares to the current implementation.\n\nIdeal sampling is not my goal, but it would be nice not to regress the current implementation.",
      "parentUuid": "9e9cff68_78149eca",
      "revId": "b0b11e0169dd33a938c67cc25512fc484e43198f",
      "serverId": "85c56323-6fa9-3386-8a01-6480fb634889"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "0e824217_31199a29",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 1057373
      },
      "writtenOn": "2024-11-06T21:42:15Z",
      "side": 1,
      "message": "I was able to reproduce the biased results pretty easily by spawning a bunch of threads and allocating at a few different call sites first thing.\n\nI updated the CL to force the slow path allocation every time and report object allocation size. That fixes the bias issues, and appears to be not too bad at all performance wise compared to the cost of taking samples when profiling is enabled.",
      "parentUuid": "ab03c885_bef8b9d7",
      "revId": "b0b11e0169dd33a938c67cc25512fc484e43198f",
      "serverId": "85c56323-6fa9-3386-8a01-6480fb634889"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "8e6de458_f87cc244",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 1042828
      },
      "writtenOn": "2024-11-09T01:03:36Z",
      "side": 1,
      "message": "How does the sampling work? It turns profiling on and off? Once you get into AHeapProfile_reportAllocation, this looks pretty heavy-weight?",
      "parentUuid": "0e824217_31199a29",
      "revId": "b0b11e0169dd33a938c67cc25512fc484e43198f",
      "serverId": "85c56323-6fa9-3386-8a01-6480fb634889"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "dbbda708_9a3c3c12",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 1057373
      },
      "writtenOn": "2024-11-11T18:53:49Z",
      "side": 1,
      "message": "AHeapProfile_reportAllocation does sampling internally based on the sampling interval. https://perfetto.dev/docs/design-docs/heapprofd-sampling#implementation.\n\nIt only takes a stack trace sample if the \"computed number of times we would draw a sample if we sampled each byte with the given sampling rate\" is greater than zero.\n\nThere is going to be some performance cost to profiling. There are two things to worry about:\n1. The performance cost of taking the sample, which can be reduced by increasing the sample interval.\n2. The performance cost of having profiling enabled without taking the sample (e.g. runtime overhead when sample interval is extremely large).\n\nI don\u0027t think this CL regresses the performance cost of (1) significantly (though I haven\u0027t looked in detail). This CL does regress (2) a bit. How important is it to reduce the performance costs of profiling?",
      "parentUuid": "8e6de458_f87cc244",
      "revId": "b0b11e0169dd33a938c67cc25512fc484e43198f",
      "serverId": "85c56323-6fa9-3386-8a01-6480fb634889"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "23727bde_53bed65d",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 1386032
      },
      "writtenOn": "2024-11-12T11:47:39Z",
      "side": 1,
      "message": "I am not the owner (or familiar) with this part of the codebase so I\u0027ve cc\u0027d @ddiproietto@google.com that might have some more inputs.\n\nfwiw the perfetto profilers have historically tried to minimize impact on the profiled process - e.g. out-of-process unwinding, sampling, non-blocking sends to heapprofd so that we can confidently enable them without degrading user-perceptible performance",
      "parentUuid": "dbbda708_9a3c3c12",
      "revId": "b0b11e0169dd33a938c67cc25512fc484e43198f",
      "serverId": "85c56323-6fa9-3386-8a01-6480fb634889"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "75d996f2_e8a9aad8",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 1042828
      },
      "writtenOn": "2024-11-13T19:28:25Z",
      "side": 1,
      "message": "My guess is that this would decrease the performance of small object allocation with an essentially infinite sampling interval by an order of magnitude or so. It doesn\u0027t affect the associated GC cost, but I would still expect that code that\u0027s dominated by small object allocation would slow down by at least a factor of two or so, possibly significantly more. Of course actual measurements would be a lot more convincing.\n\n(I actually don\u0027t fully understand the cost impact here for various configurations. It seems that in the partial TLAB case, we could at least slowly increment the size of the TLAB, without the full allocate-a-new-TLAB each time dance. The latter requires several lock acquisitions, etc., which would also destroy our ability to allocate in multiple threads in parallel. The former probably not. But I don\u0027t think the CMC collector currently uses partial TLABs.)\n\nMy intuition is that this will sometimes cause a user-perceptible slowdown. If you only enable it a fraction of a second at a time, it may be OK. But otherwise bypassing the allocation fast path completely, as I think we\u0027re doing here, seems heavy. We\u0027re replacing a short hand-written assembly routine with a full entry into the runtime, which then executes a fair amount of code, both to allocate, and to test whether we should sample.\n\nI think the old scheme sort-of was (and probably should have been):\n\n- Randomize the TLAB size\n- on TLAB overflow, count that allocation as (TLAB size / allocation size) samples of that size.\n- Non-TLAB allocations are sampled directly.\n\nThat doesn\u0027t quite work, especially for largish TLABs, because it fails to count TLABs that are retired without overflowing, e.g. because the thread is short-lived, or we GC before the TLAB fills. So we somehow need to sample those as well. But I think it should suffice to sample a random location within such a TLAB before we retire it, and treat it the same way. And since we have a bitmap for object boundaries, I think we can do that. I\u0027m not sure we currently do that, and that may be part of the problem. And we\u0027re definitely not consistent about non-TLAB allocations.\n\nIf we agree that\u0027s reasonable, does it make sense to work towards this model? It doesn\u0027t actually seem that complicated to me. Do we have functionality on the Perfetto side to, say, record the fact that we just allocated 1000 24 byte objects, without going through the motions 1000 times? I think it would be better if that code were in Perfetto than in ART, since I think long-term more of the statistics expertise is likely to be there.",
      "parentUuid": "23727bde_53bed65d",
      "revId": "b0b11e0169dd33a938c67cc25512fc484e43198f",
      "serverId": "85c56323-6fa9-3386-8a01-6480fb634889"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "2184cbb7_b1943f79",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 1057373
      },
      "writtenOn": "2024-11-13T20:21:06Z",
      "side": 1,
      "message": "I wrote a test that does 400,400 allocations totaling 307,609,600 bytes, a mix of 512 and 1024 long byte arrays.\n\nWithout JHP enabled, the test takes 0.11 seconds to complete.\nWith JHP enabled and infinite sampling interval, the existing implementation takes 0.13s to complete the test. Patch set 2 of this CL takes 0.4s to complete the test. That suggest about 4x overhead of having JHP enabled with infinite sampling interval. (When sampling interval isn\u0027t infinite and sampling cost takes over, I see my test taking on the order of 1 or 2 seconds to complete with and without my change.)\n\nI\u0027m happy to work towards a model that randomizes the TLAB size to try and get the overhead of JHP enabled down to what it was before. Let me work on a draft for that.",
      "parentUuid": "75d996f2_e8a9aad8",
      "revId": "b0b11e0169dd33a938c67cc25512fc484e43198f",
      "serverId": "85c56323-6fa9-3386-8a01-6480fb634889"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "af9230cc_cc4e05a6",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 1042828
      },
      "writtenOn": "2024-11-14T00:00:01Z",
      "side": 1,
      "message": "My concern is that our average allocation size is probably much smaller than this? The GC cost is proportional to bytes allocated, but the allocation overhead will be largely proportional to the number of objects allocated. Is it easy to retry this experiment with 16-byte, e.g. Long objects? (16-bytes is clearly below average. But it would allow us to bracket the actual costs.)",
      "parentUuid": "2184cbb7_b1943f79",
      "revId": "b0b11e0169dd33a938c67cc25512fc484e43198f",
      "serverId": "85c56323-6fa9-3386-8a01-6480fb634889"
    }
  ]
}